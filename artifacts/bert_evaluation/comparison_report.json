{
  "comparison_summary": {
    "model_versions": {
      "baseline": "Traditional ML (TF-IDF + LogisticRegression)",
      "bert": "BERT Fine-tuned (dbmdz/bert-base-turkish-cased)"
    },
    "comparison_date": "2025-08-11T02:15:53.986616",
    "overall_winner": "BERT"
  },
  "detailed_comparison": {
    "overall": {
      "accuracy": {
        "baseline": 0.6739139091307291,
        "bert": 0.8017260703878054,
        "improvement": 0.12781216125707628
      },
      "f1_macro": {
        "baseline": 0,
        "bert": 0.8012342928974413,
        "improvement": 0.8012342928974413
      }
    },
    "per_class": {
      "Negative": {
        "baseline": 0,
        "bert": 0,
        "improvement": 0
      },
      "Neutral": {
        "baseline": 0,
        "bert": 0,
        "improvement": 0
      },
      "Positive": {
        "baseline": 0,
        "bert": 0,
        "improvement": 0
      }
    }
  },
  "raw_metrics": {
    "baseline": {
      "accuracy": 0.6739139091307291,
      "confusion_matrix": [
        [
          8596,
          1987,
          176
        ],
        [
          1037,
          2615,
          460
        ],
        [
          554,
          3975,
          5713
        ]
      ],
      "test_date": "2025-08-11 02:15:43"
    },
    "bert": {
      "accuracy": 0.8017260703878054,
      "f1_macro": 0.8012342928974413,
      "f1_weighted": 0.8020315313006509,
      "average_confidence": 0.8373256325721741,
      "classification_report": {
        "LABEL_0": {
          "precision": 0.8555790960451978,
          "recall": 0.8474991255683806,
          "f1-score": 0.8515199437708663,
          "support": 8577.0
        },
        "LABEL_1": {
          "precision": 0.8463583200425305,
          "recall": 0.843309672634813,
          "f1-score": 0.8448312460199533,
          "support": 9439.0
        },
        "LABEL_2": {
          "precision": 0.7027636773829667,
          "recall": 0.712,
          "f1-score": 0.7073516889015043,
          "support": 8750.0
        },
        "accuracy": 0.8017260703878054,
        "macro avg": {
          "precision": 0.8015670311568983,
          "recall": 0.8009362660677312,
          "f1-score": 0.8012342928974413,
          "support": 26766.0
        },
        "weighted avg": {
          "precision": 0.8023709282956761,
          "recall": 0.8017260703878054,
          "f1-score": 0.8020315313006509,
          "support": 26766.0
        }
      },
      "confusion_matrix": [
        [
          7269,
          94,
          1214
        ],
        [
          58,
          7960,
          1421
        ],
        [
          1169,
          1351,
          6230
        ]
      ],
      "per_class_metrics": {
        "LABEL_0": {
          "precision": 0.8555790960451978,
          "recall": 0.8474991255683806,
          "f1-score": 0.8515199437708663,
          "support": 8577.0
        },
        "LABEL_1": {
          "precision": 0.8463583200425305,
          "recall": 0.843309672634813,
          "f1-score": 0.8448312460199533,
          "support": 9439.0
        },
        "LABEL_2": {
          "precision": 0.7027636773829667,
          "recall": 0.712,
          "f1-score": 0.7073516889015043,
          "support": 8750.0
        }
      }
    }
  }
}