#BERTurk fine-tuning hyperparameters

model: 
  name: "dbmdz/bert-base-turkish-cased"
  max_length: 512
  num_classes: 3

training:
  batch_size: 8
  eval_batch_size: 16
  learning_rate: 2e-5
  num_epochs: 5
  warmup_steps: 500
  weight_decay: 0.01
  gradient_accumulation_steps: 4
  fp16: true
  dataloader_pin_memory: true

early_stopping:
  patience: 3
  threshold: 0.001
  metric_for_best_model: "eval_f1"
  greater_is_better: true

evaluation:
  eval_steps: 500
  logging_steps: 100
  save_steps: 1000
  load_best_model_at_end: true

calibration:
  method: "platt"
  validation_split: 0.2

export:
  onnx_optimization: true
  target_size_mb: 140