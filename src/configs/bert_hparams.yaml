  #BERTurk fine-tuning hyperparameters
      #Low Learning Rate (2e-5): BERT is pre-trained, so fine-tuning 
              #requires very small learning rates to avoid catastrophic forgetting.

model: 
  name: "dbmdz/bert-base-turkish-uncased"
  max_length: 256
  num_classes: 3

training:
  batch_size: 16
  eval_batch_size: 32
  learning_rate: 2e-5
  num_epochs: 2 # means train for 3 epochs. epoch means one pass through the entire dataset.
  warmup_steps: 500
  weight_decay: 0.01
  gradient_accumulation_steps: 2
  fp16: true
  dataloader_pin_memory: true
  class_weights:
    0: 1  # Negative
    1: 1  # Positive  
    2: 1.25  # Neutral

early_stopping:
  patience: 20 # means stop if no improvement for 3 epochs
  threshold: 0.001
  metric_for_best_model: "eval_f1"
  greater_is_better: true

evaluation:
  eval_steps: 500
  logging_steps: 100
  save_steps: 1000
  load_best_model_at_end: true

calibration:
  method: "platt"
  validation_split: 0.2

export:
  onnx_optimization: true
  target_size_mb: 140